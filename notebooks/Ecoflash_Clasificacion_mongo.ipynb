{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "124c968f",
   "metadata": {},
   "source": [
    "**Importacion de librerias**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0081b639",
   "metadata": {},
   "source": [
    "**Conectamos a MongoDB Atlas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f68a323",
   "metadata": {},
   "outputs": [],
   "source": [
    "## librerias y paquetes\n",
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import io\n",
    "import requests\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pymongo import MongoClient\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ceb1f060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîó Conectando a MongoDB Atlas...\n",
      "Docs cargados: 3068\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "print(\"\\nüîó Conectando a MongoDB Atlas...\")\n",
    "# Conexi√≥n a MongoDB Atlas\n",
    "uri = \"mongodb+srv://jonnathanftigreest_db_user:j3nhScPaM7SpNacc@cluster0.lckuzqv.mongodb.net/?appName=Cluster0\"\n",
    "client = MongoClient(uri)\n",
    "\n",
    "db = client[\"EcoFlash\"]\n",
    "collection = db[\"images\"]\n",
    "\n",
    "docs = list(collection.find({}))\n",
    "\n",
    "print(\"Docs cargados:\", len(docs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4f4c9d",
   "metadata": {},
   "source": [
    "## descarga del data_set para optimizacion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cd1bbf",
   "metadata": {},
   "source": [
    "**Configuraciones Generales**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71f77275",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "TARGET_SIZE = (224, 224)\n",
    "VALIDATION_SPLIT = 0.15\n",
    "SEED = 133\n",
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a0cff97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíª PYTORCH: 2.9.0+cu130\n",
      "CUDA: True\n",
      "GPU: NVIDIA GeForce RTX 3050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"üíª PYTORCH:\", torch.__version__)\n",
    "print(\"CUDA:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e6ba84",
   "metadata": {},
   "source": [
    "## Definici√≥n del Dataset MongoDB+GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a114944f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GitHubMongoDataset(Dataset):\n",
    "    def __init__(self, mongo_collection, transform=None, cache_dir=\"cache_images\"):\n",
    "        self.docs = list(mongo_collection.find({}))\n",
    "        self.transform = transform\n",
    "        self.cache_dir = cache_dir\n",
    "\n",
    "        if not os.path.exists(cache_dir):\n",
    "            os.makedirs(cache_dir)\n",
    "\n",
    "        # Extraer clases √∫nicas\n",
    "        self.classes = sorted(list({doc[\"category\"] for doc in self.docs}))\n",
    "        print(\"Clases detectadas:\", self.classes)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.docs)\n",
    "\n",
    "    def _label_to_int(self, label):\n",
    "        return self.classes.index(label)\n",
    "\n",
    "    def _download(self, url, cache_path):\n",
    "        try:\n",
    "            r = requests.get(url, timeout=10)\n",
    "            r.raise_for_status()\n",
    "            open(cache_path, \"wb\").write(r.content)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(\"Error descargando:\", url)\n",
    "            return False\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        doc = self.docs[idx]\n",
    "\n",
    "        label = self._label_to_int(doc[\"category\"])\n",
    "        url = doc[\"url\"]\n",
    "        filename = url.split(\"/\")[-1]\n",
    "        cache_path = f\"{self.cache_dir}/{filename}\"\n",
    "\n",
    "        if not os.path.exists(cache_path):\n",
    "            ok = self._download(url, cache_path)\n",
    "            if not ok:\n",
    "                raise RuntimeError(\"No se pudo descargar la imagen.\")\n",
    "\n",
    "        img = Image.open(cache_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d04fed",
   "metadata": {},
   "source": [
    "## Transformaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "528c0966",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize(TARGET_SIZE),\n",
    "    transforms.RandomHorizontalFlip(0.5),\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.ColorJitter(0.2, 0.2, 0.2),\n",
    "    transforms.RandomAffine(\n",
    "        degrees=0,\n",
    "        translate=(0.1, 0.1),\n",
    "        scale=(0.9, 1.1),\n",
    "        shear=5\n",
    "    ),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=(0.485, 0.456, 0.406),\n",
    "        std=(0.229, 0.224, 0.225)\n",
    "    )\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize(TARGET_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=(0.485, 0.456, 0.406),\n",
    "        std=(0.229, 0.224, 0.225)\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5e8cfe",
   "metadata": {},
   "source": [
    "## Crear dataset completo desde Mongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d35e38a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clases detectadas: ['.git', 'cardboard', 'glass', 'metal', 'paper', 'plastic', 'trash']\n",
      "Total samples: 3068\n",
      "Total classes: ['.git', 'cardboard', 'glass', 'metal', 'paper', 'plastic', 'trash']\n"
     ]
    }
   ],
   "source": [
    "full_dataset = GitHubMongoDataset(collection, transform=None)\n",
    "\n",
    "labels = [full_dataset._label_to_int(doc[\"category\"]) for doc in full_dataset.docs]\n",
    "\n",
    "classes = full_dataset.classes\n",
    "num_classes = len(classes)\n",
    "\n",
    "print(\"Total samples:\", len(full_dataset))\n",
    "print(\"Total classes:\", classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305cea0c",
   "metadata": {},
   "source": [
    "## Split Estratificado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65dc8f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clases detectadas: ['.git', 'cardboard', 'glass', 'metal', 'paper', 'plastic', 'trash']\n",
      "Clases detectadas: ['.git', 'cardboard', 'glass', 'metal', 'paper', 'plastic', 'trash']\n",
      "Train: 2607 Val: 461\n"
     ]
    }
   ],
   "source": [
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=VALIDATION_SPLIT, random_state=SEED)\n",
    "\n",
    "train_idx, val_idx = next(sss.split(np.arange(len(labels)), labels))\n",
    "\n",
    "train_dataset = Subset(\n",
    "    GitHubMongoDataset(collection, transform=train_transforms),\n",
    "    train_idx\n",
    ")\n",
    "\n",
    "val_dataset = Subset(\n",
    "    GitHubMongoDataset(collection, transform=val_transforms),\n",
    "    val_idx\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "print(\"Train:\", len(train_dataset), \"Val:\", len(val_dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f02e09",
   "metadata": {},
   "source": [
    "## Construcci√≥n del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7425a67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.unique(labels),\n",
    "    y=labels\n",
    ")\n",
    "weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "num_features = model.fc.in_features\n",
    "\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(num_features, num_classes)\n",
    ")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if name.startswith((\"layer2\", \"layer3\", \"layer4\", \"fc\")):\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "\n",
    "model = model.to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b877150",
   "metadata": {},
   "source": [
    "## CONFIGURACI√ìN DE OPTIMIZADOR Y SCHEDULER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6af2eca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(weight=weights_tensor)\n",
    "# CAMBIO: AdamW es m√°s estable que Adam cl√°sico\n",
    "optimizer = optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    patience=5,\n",
    "    factor=0.1,\n",
    "    min_lr=1e-5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad95c97c",
   "metadata": {},
   "source": [
    "# ENTRENAMIENTO DEL MODELO (EARLY STOPPING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dcf9ed0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Early Stopping\n",
    "early_stopping_patience = 10\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "best_model_path = \"trash_resnet50_best_v3.pth\"   # CAMBIO: nuevo nombre versi√≥n 3\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "train_accuracies, val_accuracies = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b349f0da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== √âpoca 1/50 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando [1]:  38%|‚ñà‚ñà‚ñà‚ñä      | 31/82 [05:24<08:53, 10.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error descargando: https://raw.githubusercontent.com/tigreraph/ecoflash-dataset/main/.git/COMMIT_EDITMSG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No se pudo descargar la imagen.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m model.train()\n\u001b[32m     12\u001b[39m train_loss, train_correct, n_train = \u001b[32m0.0\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mxb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mEntrenando [\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mepoch\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m]\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myb\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mxb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jonna\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jonna\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:732\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    736\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    738\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jonna\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:788\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    787\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m788\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    789\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    790\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jonna\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:50\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.auto_collation:\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.dataset, \u001b[33m\"\u001b[39m\u001b[33m__getitems__\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     52\u001b[39m         data = [\u001b[38;5;28mself\u001b[39m.dataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jonna\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:416\u001b[39m, in \u001b[36mSubset.__getitems__\u001b[39m\u001b[34m(self, indices)\u001b[39m\n\u001b[32m    414\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__([\u001b[38;5;28mself\u001b[39m.indices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m    415\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m416\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 41\u001b[39m, in \u001b[36mGitHubMongoDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     39\u001b[39m     ok = \u001b[38;5;28mself\u001b[39m._download(url, cache_path)\n\u001b[32m     40\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ok:\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo se pudo descargar la imagen.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     43\u001b[39m img = Image.open(cache_path).convert(\u001b[33m\"\u001b[39m\u001b[33mRGB\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform:\n",
      "\u001b[31mRuntimeError\u001b[39m: No se pudo descargar la imagen."
     ]
    }
   ],
   "source": [
    "# === ENTRENAMIENTO ===\n",
    "from torch import device\n",
    "\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    print(f\"\\n===== √âpoca {epoch}/{EPOCHS} =====\")\n",
    "\n",
    "    # ============================================================\n",
    "    # ENTRENAMIENTO\n",
    "    # ============================================================\n",
    "    model.train()\n",
    "    train_loss, train_correct, n_train = 0.0, 0, 0\n",
    "\n",
    "    for xb, yb in tqdm(train_loader, desc=f\"Entrenando [{epoch}]\"):\n",
    "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(xb)\n",
    "        loss = criterion(out, yb)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * xb.size(0)\n",
    "        train_correct += (out.argmax(1) == yb).sum().item()\n",
    "        n_train += xb.size(0)\n",
    "\n",
    "    train_loss /= n_train\n",
    "    train_acc = train_correct / n_train\n",
    "\n",
    "    # ============================================================\n",
    "    # VALIDACI√ìN\n",
    "    # ============================================================\n",
    "    model.eval()\n",
    "    val_loss, val_correct, n_val = 0.0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in tqdm(val_loader, desc=f\"Validando [{epoch}]\"):\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "\n",
    "            out = model(xb)\n",
    "            loss = criterion(out, yb)\n",
    "\n",
    "            val_loss += loss.item() * xb.size(0)\n",
    "            val_correct += (out.argmax(1) == yb).sum().item()\n",
    "            n_val += xb.size(0)\n",
    "\n",
    "    val_loss /= n_val\n",
    "    val_acc = val_correct / n_val\n",
    "\n",
    "    # ============================================================\n",
    "    # ACTUALIZAR SCHEDULER\n",
    "    # ============================================================\n",
    "    scheduler.step(val_loss)  # CAMBIO: AdamW + ReduceLROnPlateau mejor integrados\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "    print(f\"Learning Rate actual: {current_lr:.6f}\")\n",
    "    print(f\"Entrenamiento ‚Äî Loss: {train_loss:.4f} | Acc: {train_acc:.3f}\")\n",
    "    print(f\"Validaci√≥n   ‚Äî Loss: {val_loss:.4f} | Acc: {val_acc:.3f}\")\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    val_accuracies.append(val_acc)\n",
    "\n",
    "    # ============================================================\n",
    "    # EARLY STOPPING (MISMA L√ìGICA, M√ÅS ROBUSTA)\n",
    "    # ============================================================\n",
    "    if val_loss < best_val_loss - 1e-4:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_no_improve = 0\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f\"‚úÖ Modelo mejorado guardado: {best_model_path}\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        print(f\"‚è≥ No mejora ({epochs_no_improve}/{early_stopping_patience})\")\n",
    "\n",
    "        if epochs_no_improve >= early_stopping_patience:\n",
    "            print(\"‚õî Early Stopping activado.\")\n",
    "            break\n",
    "\n",
    "# ======================================================\n",
    "# === CARGAR EL MEJOR MODELO GUARDADO ===\n",
    "# ======================================================\n",
    "model.load_state_dict(torch.load(best_model_path, map_location=DEVICE))\n",
    "print(\"\\n‚úÖ Entrenamiento finalizado. Mejor modelo cargado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0069d04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar mejor modelo\n",
    "model.load_state_dict(torch.load(best_model_path, map_location=DEVICE))\n",
    "torch.save(model.state_dict(), \"resnet50_trash_final_v3.pt\")\n",
    "print(\"üéâ Entrenamiento completado y modelo final guardado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4af95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluacion Modelo\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for xb, yb in val_loader:\n",
    "        xb = xb.to(DEVICE)\n",
    "        preds = model(xb).argmax(1).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(yb.numpy())\n",
    "\n",
    "print(\"\\nüìä Reporte de clasificaci√≥n:\\n\")\n",
    "print(classification_report(all_labels, all_preds, target_names=full_dataset.classes))\n",
    "# Matriz de confusi√≥n\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, cmap=\"Blues\",\n",
    "            xticklabels=full_dataset.classes,\n",
    "            yticklabels=full_dataset.classes)\n",
    "plt.title(\"Matriz de Confusi√≥n\")\n",
    "plt.xlabel(\"Predicci√≥n\")\n",
    "plt.ylabel(\"Real\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0700e77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Gr√°ficas de m√©tricas por √©poca ===\n",
    "epochs_range = range(len(train_accuracies))\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "# --- Accuracy ---\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(epochs_range, train_accuracies, label='Entrenamiento', marker='x')\n",
    "plt.plot(epochs_range, val_accuracies, label='Validaci√≥n', marker='x')\n",
    "plt.title('Accuracy vs. No. of epochs')\n",
    "plt.xlabel('√âpoca')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# --- Loss ---\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(epochs_range, train_losses, label='Entrenamiento', marker='x')\n",
    "plt.plot(epochs_range, val_losses, label='Validaci√≥n', marker='x')\n",
    "plt.title('Loss vs. No. of epochs')\n",
    "plt.xlabel('√âpoca')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad6c353",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_macro = f1_score(all_labels, all_preds, average='macro')\n",
    "print(f\"\\nF1 Macro Score: {f1_macro:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cf5dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_model = models.resnet50(weights=None)\n",
    "infer_model.fc = nn.Sequential(\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(num_features, num_classes)\n",
    ")\n",
    "infer_model.load_state_dict(\n",
    "    torch.load(\"resnet50_trash_final_v3.pt\", map_location=DEVICE)\n",
    ")\n",
    "infer_model = infer_model.to(DEVICE)\n",
    "infer_model.eval()\n",
    "\n",
    "infer_transforms = val_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc10963",
   "metadata": {},
   "outputs": [],
   "source": [
    "## funci√≥n para predecir imagen individual\n",
    "def predict_image(img_path):\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    x = infer_transforms(img).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        probs = torch.softmax(infer_model(x), dim=1)[0]\n",
    "        pred_idx = probs.argmax().item()\n",
    "\n",
    "    print(\"\\nüß† Predicci√≥n:\", full_dataset.classes[pred_idx])\n",
    "    print(\"\\nProbabilidades:\")\n",
    "    for cls, p in zip(full_dataset.classes, probs):\n",
    "        print(f\"{cls:10s}: {p.item()*100:.2f}%\")\n",
    "\n",
    "    plt.imshow(img)\n",
    "    plt.title(full_dataset.classes[pred_idx])\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b2ba41",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_image(\"test/test4.jpeg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

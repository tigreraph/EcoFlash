# -*- coding: utf-8 -*-
"""Ecoflash_Clasificacion.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ozi6psQnzB3ET-Hm6Mu6TrDBm_a8wE19

**Importacion de librerias**
"""

import os
import warnings
from pathlib import Path

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.utils.class_weight import compute_class_weight
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.model_selection import StratifiedShuffleSplit

from pymongo import MongoClient

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset

from torchvision import transforms, models
from PIL import Image
from tqdm import tqdm

warnings.filterwarnings("ignore")

"""**Conectamos a MongoDB Atlas**"""

from pymongo import MongoClient
print("\nüîó Conectando a MongoDB Atlas...")
# Conexi√≥n a MongoDB Atlas
uri = "mongodb+srv://jonnathanftigreest_db_user:j3nhScPaM7SpNacc@cluster0.lckuzqv.mongodb.net/?appName=Cluster0"
client = MongoClient(uri)

db = client["clasificacion_imagenes"]
collection = db["data_image"]

docs = list(collection.find({}))

print("Docs cargados:", len(docs))

"""**Configuraciones Generales**"""

from PIL import Image

path = docs[1000]["filepath"]
print(path)
Image.open(path)

import os
print(os.path.exists(path))

BATCH_SIZE = 32
TARGET_SIZE = (224, 224)
VALIDATION_SPLIT = 0.15
SEED = 133
EPOCHS = 50

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("üíª PYTORCH:", torch.__version__)
print("CUDA:", torch.cuda.is_available())
if torch.cuda.is_available():
    print("GPU:", torch.cuda.get_device_name(0))

"""**Exploraci√≥n del Dataset y Transformaciones**

**Transformaciones**
"""

### dataset personalizado de mongo
class MongoImageDataset(Dataset):
    def __init__(self, documents, transform=None):
        self.docs = documents
        self.transform = transform

        self.classes = sorted(list({doc["category"] for doc in self.docs}))
        self.class_to_idx = {cls: i for i, cls in enumerate(self.classes)}

    def __len__(self):
        return len(self.docs)

    def __getitem__(self, idx):
        doc = self.docs[idx]

        img_path = doc["filepath"]
        label = self.class_to_idx[doc["category"]]

        img = Image.open(img_path).convert("RGB")
        if self.transform:
            img = self.transform(img)

        return img, label

"""**TRANSFORMACIONES DE IMAGEN (AUGMENTATION)**"""

train_transforms = transforms.Compose([
    transforms.Resize(TARGET_SIZE),
    transforms.RandomHorizontalFlip(0.5),
    transforms.RandomRotation(20),
    transforms.ColorJitter(0.2, 0.2, 0.2),
    transforms.RandomAffine(
        degrees=0,
        translate=(0.1, 0.1),
        scale=(0.9, 1.1),
        shear=5
    ),
    transforms.ToTensor(),
    transforms.Normalize(mean=(0.485, 0.456, 0.406),
                         std=(0.229, 0.224, 0.225))
])

val_transforms = transforms.Compose([
    transforms.Resize(TARGET_SIZE),
    transforms.ToTensor(),
    transforms.Normalize(mean=(0.485, 0.456, 0.406),
                         std=(0.229, 0.224, 0.225))
])

"""**SPLIT ESTRATIFICADO TRAIN / VALIDATION**"""

labels = [doc["category"] for doc in docs]

sss = StratifiedShuffleSplit(n_splits=1, test_size=VALIDATION_SPLIT, random_state=SEED)
train_idx, val_idx = next(sss.split(docs, labels))

train_docs = [docs[i] for i in train_idx]
val_docs = [docs[i] for i in val_idx]

train_dataset = MongoImageDataset(train_docs, transform=train_transforms)
val_dataset = MongoImageDataset(val_docs, transform=val_transforms)

num_workers = 2 if torch.cuda.is_available() else 0

train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,
                          num_workers=num_workers, pin_memory=True)

val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False,
                        num_workers=num_workers, pin_memory=True)

print(f"üìä Train: {len(train_dataset)} | Validation: {len(val_dataset)}")
print("Clases detectadas:", train_dataset.classes)

"""**Balanceo de Clases**"""

class_weights = compute_class_weight(
    class_weight="balanced",
    classes=np.unique(labels),
    y=labels
)

weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(DEVICE)
print("‚öñÔ∏è Pesos de clase:", weights_tensor)

"""**CONSTRUCCI√ìN DEL MODELO RESNET50**"""

model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)

num_classes = len(train_dataset.classes)
num_features = model.fc.in_features

model.fc = nn.Sequential(
    nn.Dropout(0.5),
    nn.Linear(num_features, num_classes)
)

for name, param in model.named_parameters():
    if name.startswith(("layer2", "layer3", "layer4", "fc")):
        param.requires_grad = True
    else:
        param.requires_grad = False

model = model.to(DEVICE)

"""**CONFIGURACI√ìN DE OPTIMIZADOR Y SCHEDULER**"""

criterion = nn.CrossEntropyLoss(weight=weights_tensor)

optimizer = optim.AdamW(
    filter(lambda p: p.requires_grad, model.parameters()),
    lr=1e-3,
    weight_decay=1e-4
)

scheduler = optim.lr_scheduler.ReduceLROnPlateau(
    optimizer,
    patience=5,
    factor=0.1,
    min_lr=1e-5
)

"""**ENTRENAMIENTO DEL MODELO (EARLY STOPPING)**"""

early_stopping_patience = 10
best_val_loss = float('inf')
epochs_no_improve = 0
best_model_path = "mongo_resnet50_best.pth"
train_losses, val_losses = [], []
train_accuracies, val_accuracies = [], []

print("\nüöÄ Iniciando entrenamiento...\n")

for epoch in range(1, EPOCHS + 1):
    print(f"\n===== √âpoca {epoch}/{EPOCHS} =====")

    # ------------------ ENTRENAMIENTO ------------------
    model.train()
    train_loss, train_correct, n_train = 0.0, 0, 0

    for xb, yb in tqdm(train_loader, desc=f"Entrenando [{epoch}]"):
        xb, yb = xb.to(DEVICE), yb.to(DEVICE)

        optimizer.zero_grad()
        out = model(xb)
        loss = criterion(out, yb)

        loss.backward()
        optimizer.step()

        train_loss += loss.item() * xb.size(0)
        train_correct += (out.argmax(1) == yb).sum().item()
        n_train += xb.size(0)

    train_loss /= n_train
    train_acc = train_correct / n_train

    # ------------------- VALIDACI√ìN --------------------
    model.eval()
    val_loss, val_correct, n_val = 0.0, 0, 0

    with torch.no_grad():
        for xb, yb in tqdm(val_loader, desc=f"Validando [{epoch}]"):
            xb, yb = xb.to(DEVICE), yb.to(DEVICE)

            out = model(xb)
            loss = criterion(out, yb)

            val_loss += loss.item() * xb.size(0)
            val_correct += (out.argmax(1) == yb).sum().item()
            n_val += xb.size(0)

    val_loss /= n_val
    val_acc = val_correct / n_val

    scheduler.step(val_loss)

    print(f"Learning Rate actual: {optimizer.param_groups[0]['lr']:.6f}")
    print(f"Entrenamiento ‚Äî Loss: {train_loss:.4f} | Acc: {train_acc:.3f}")
    print(f"Validaci√≥n   ‚Äî Loss: {val_loss:.4f} | Acc: {val_acc:.3f}")

    train_losses.append(train_loss)
    val_losses.append(val_loss)
    train_accuracies.append(train_acc)
    val_accuracies.append(val_acc)

    # ------------------ EARLY STOPPING ------------------
    if val_loss < best_val_loss - 1e-4:
        best_val_loss = val_loss
        epochs_no_improve = 0
        torch.save(model.state_dict(), best_model_path)
        print(f"üíæ Modelo mejorado guardado: {best_model_path}")

    else:
        epochs_no_improve += 1
        print(f"‚è≥ Sin mejora ({epochs_no_improve}/{early_stopping_patience})")

        if epochs_no_improve >= early_stopping_patience:
            print("‚õî Early Stopping activado.")
            break

from PIL import Image

path = docs[0]["filepath"]
print(path)
Image.open(path)

"""**EVALUACI√ìN DEL MODELO**"""
# -*- coding: utf-8 -*-
"""eco_flash_clasificacion_local.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FTQYRRe0fypGu1QSwnppOs57i-AWQs5C
"""

import os
import warnings
from pathlib import Path

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.utils.class_weight import compute_class_weight
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.model_selection import StratifiedShuffleSplit

import torch
import torch.nn as nn
import torch.optim as optim

from torch.utils.data import DataLoader, Subset
from torchvision import datasets, transforms, models
from PIL import Image
from tqdm import tqdm

warnings.filterwarnings("ignore")

"""
# PASO 2: CONFIGURACIONES GENERALES
"""

DATA_PATH = "trash_images"  # CAMBIO: antes usabas "trash_images"
BATCH_SIZE = 32
TARGET_SIZE = (224, 224)  # CAMBIO: tama√±o √≥ptimo para ResNet (antes 180x180)
VALIDATION_SPLIT = 0.15
SEED = 133
EPOCHS = 50

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

print("Versi√≥n de PyTorch:", torch.__version__)
print("CUDA:", torch.cuda.is_available())
if torch.cuda.is_available():
    print("GPU:", torch.cuda.get_device_name(0))

"""# PASO 3: EXPLORACI√ìN Y PREPARACI√ìN DEL DATASET"""

main_folder_path = Path(DATA_PATH)

all_folders = [d for d in main_folder_path.glob('**/') if d.is_dir()]

# Count number of files in each class
data = []
for folder in all_folders:
    folder_name = folder.name
    file_count = len(list(folder.glob('*.*')))
    if folder_name != DATA_PATH:
        data.append({'Folder Name': folder_name, 'File Count': file_count})

count = pd.DataFrame(data)

count = count.set_index('Folder Name')
count

# funcion para mostrar las 5 primeras imagenes de cada clase
def plot_imgs(item_dir, top=10):
    all_item_dirs = os.listdir(item_dir)
    item_files = [os.path.join(item_dir, file) for file in all_item_dirs][:5]

    plt.figure(figsize=(10, 10))

    for idx, img_path in enumerate(item_files):
        plt.subplot(5, 5, idx+1)

        img = plt.imread(img_path)
        plt.tight_layout()
        plt.imshow(img, cmap='gray')
        plt.axis('off')
        plt.title(os.path.basename(item_dir))

## muestra de imagenes por clase
plot_imgs(DATA_PATH+'/cardboard')
plot_imgs(DATA_PATH+'/glass')
plot_imgs(DATA_PATH+'/metal')
plot_imgs(DATA_PATH+'/paper')
plot_imgs(DATA_PATH+'/plastic')
plot_imgs(DATA_PATH+'/trash')

"""# PASO 4: TRANSFORMACIONES DE IMAGEN"""

train_transforms = transforms.Compose([
    transforms.Resize(TARGET_SIZE),
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.RandomRotation(20),      # CAMBIO: antes 40¬∞
    transforms.ColorJitter(
        brightness=0.2,
        contrast=0.2,
        saturation=0.2
    ),
    transforms.RandomAffine(
        degrees=0,
        translate=(0.1, 0.1),           # CAMBIO: antes 0.3
        scale=(0.9, 1.1),
        shear=5                         # CAMBIO: antes 25
    ),
    transforms.ToTensor(),
    transforms.Normalize(
        mean=(0.485, 0.456, 0.406),
        std=(0.229, 0.224, 0.225)
    )
])

val_transforms = transforms.Compose([
    transforms.Resize(TARGET_SIZE),
    transforms.ToTensor(),
    transforms.Normalize(
        mean=(0.485, 0.456, 0.406),
        std=(0.229, 0.224, 0.225)
    )
])

# Dataset principal
full_dataset = datasets.ImageFolder(root=DATA_PATH)
labels = [label for _, label in full_dataset.samples]

print("Clases detectadas:", full_dataset.classes)

"""# PASO 5: SPLIT ESTRATIFICADO TRAIN / VALIDATION"""

sss = StratifiedShuffleSplit(
    n_splits=1,
    test_size=VALIDATION_SPLIT,
    random_state=SEED
)

train_idx, val_idx = next(sss.split(np.arange(len(labels)), labels))

train_dataset = Subset(
    datasets.ImageFolder(DATA_PATH, transform=train_transforms),
    train_idx
)
val_dataset = Subset(
    datasets.ImageFolder(DATA_PATH, transform=val_transforms),
    val_idx
)

num_workers = 2 if torch.cuda.is_available() else 0
pin_memory = torch.cuda.is_available()

train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,
                          shuffle=True, num_workers=num_workers,
                          pin_memory=pin_memory)
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE,
                         shuffle=False, num_workers=num_workers,
                         pin_memory=pin_memory)

print(f"Train: {len(train_dataset)}, Validation: {len(val_dataset)}")

"""# PASO 6: CONSTRUCCI√ìN DEL MODELO RESNET50"""

# === BALANCEO DE CLASES ===
class_weights = compute_class_weight(
    class_weight="balanced",
    classes=np.unique(labels),
    y=labels
)
weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(DEVICE)

print("Pesos por clase:", weights_tensor)

# === MODELO RESNET50 ===
model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)

num_classes = len(full_dataset.classes)
num_features = model.fc.in_features

# CAMBIO: drop-out a√±adido para mejorar generalizaci√≥n
model.fc = nn.Sequential(
    nn.Dropout(0.5),
    nn.Linear(num_features, num_classes)
)

# Descongelar el layer completo de ResNet50

# Descongelar desde layer2 para un mejor ajuste (fine-tuning real)
for name, param in model.named_parameters():
    if name.startswith(("layer2", "layer3", "layer4", "fc")):
        param.requires_grad = True
    else:
        param.requires_grad = False

model = model.to(DEVICE)

"""# PASO 7: CONFIGURACI√ìN DE OPTIMIZADOR Y SCHEDULER"""

criterion = nn.CrossEntropyLoss(weight=weights_tensor)
# CAMBIO: AdamW es m√°s estable que Adam cl√°sico
optimizer = optim.AdamW(
    filter(lambda p: p.requires_grad, model.parameters()),
    lr=1e-3,
    weight_decay=1e-4
)

scheduler = optim.lr_scheduler.ReduceLROnPlateau(
    optimizer,
    patience=5,
    factor=0.1,
    min_lr=1e-5
)

"""# PASO 8: ENTRENAMIENTO DEL MODELO (EARLY STOPPING)"""

## Early Stopping
early_stopping_patience = 10
best_val_loss = float('inf')
epochs_no_improve = 0
best_model_path = "trash_resnet50_best_v3.pth"   # CAMBIO: nuevo nombre versi√≥n 3

train_losses, val_losses = [], []
train_accuracies, val_accuracies = [], []

# === ENTRENAMIENTO ===
from torch import device


for epoch in range(1, EPOCHS + 1):
    print(f"\n===== √âpoca {epoch}/{EPOCHS} =====")

    # ============================================================
    # ENTRENAMIENTO
    # ============================================================
    model.train()
    train_loss, train_correct, n_train = 0.0, 0, 0

    for xb, yb in tqdm(train_loader, desc=f"Entrenando [{epoch}]"):
        xb, yb = xb.to(DEVICE), yb.to(DEVICE)

        optimizer.zero_grad()
        out = model(xb)
        loss = criterion(out, yb)
        loss.backward()

        optimizer.step()

        train_loss += loss.item() * xb.size(0)
        train_correct += (out.argmax(1) == yb).sum().item()
        n_train += xb.size(0)

    train_loss /= n_train
    train_acc = train_correct / n_train

    # ============================================================
    # VALIDACI√ìN
    # ============================================================
    model.eval()
    val_loss, val_correct, n_val = 0.0, 0, 0

    with torch.no_grad():
        for xb, yb in tqdm(val_loader, desc=f"Validando [{epoch}]"):
            xb, yb = xb.to(DEVICE), yb.to(DEVICE)

            out = model(xb)
            loss = criterion(out, yb)

            val_loss += loss.item() * xb.size(0)
            val_correct += (out.argmax(1) == yb).sum().item()
            n_val += xb.size(0)

    val_loss /= n_val
    val_acc = val_correct / n_val

    # ============================================================
    # ACTUALIZAR SCHEDULER
    # ============================================================
    scheduler.step(val_loss)  # CAMBIO: AdamW + ReduceLROnPlateau mejor integrados
    current_lr = optimizer.param_groups[0]['lr']

    print(f"Learning Rate actual: {current_lr:.6f}")
    print(f"Entrenamiento ‚Äî Loss: {train_loss:.4f} | Acc: {train_acc:.3f}")
    print(f"Validaci√≥n   ‚Äî Loss: {val_loss:.4f} | Acc: {val_acc:.3f}")

    train_losses.append(train_loss)
    val_losses.append(val_loss)
    train_accuracies.append(train_acc)
    val_accuracies.append(val_acc)

    # ============================================================
    # EARLY STOPPING (MISMA L√ìGICA, M√ÅS ROBUSTA)
    # ============================================================
    if val_loss < best_val_loss - 1e-4:
        best_val_loss = val_loss
        epochs_no_improve = 0
        torch.save(model.state_dict(), best_model_path)
        print(f"‚úÖ Modelo mejorado guardado: {best_model_path}")
    else:
        epochs_no_improve += 1
        print(f"‚è≥ No mejora ({epochs_no_improve}/{early_stopping_patience})")

        if epochs_no_improve >= early_stopping_patience:
            print("‚õî Early Stopping activado.")
            break

# ======================================================
# === CARGAR EL MEJOR MODELO GUARDADO ===
# ======================================================
model.load_state_dict(torch.load(best_model_path, map_location=DEVICE))
print("\n‚úÖ Entrenamiento finalizado. Mejor modelo cargado.")

# Cargar mejor modelo
model.load_state_dict(torch.load(best_model_path, map_location=DEVICE))
torch.save(model.state_dict(), "resnet50_trash_final_v3.pt")
print("üéâ Entrenamiento completado y modelo final guardado.")

"""## Evaluacion Modelo"""

model.eval()
all_preds, all_labels = [], []

with torch.no_grad():
    for xb, yb in val_loader:
        xb = xb.to(DEVICE)
        preds = model(xb).argmax(1).cpu().numpy()
        all_preds.extend(preds)
        all_labels.extend(yb.numpy())

print("\nüìä Reporte de clasificaci√≥n:\n")
print(classification_report(all_labels, all_preds, target_names=full_dataset.classes))
# Matriz de confusi√≥n
cm = confusion_matrix(all_labels, all_preds)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, cmap="Blues",
            xticklabels=full_dataset.classes,
            yticklabels=full_dataset.classes)
plt.title("Matriz de Confusi√≥n")
plt.xlabel("Predicci√≥n")
plt.ylabel("Real")
plt.show()

# === Gr√°ficas de m√©tricas por √©poca ===
epochs_range = range(len(train_accuracies))

plt.figure(figsize=(12,5))

# --- Accuracy ---
plt.subplot(1,2,1)
plt.plot(epochs_range, train_accuracies, label='Entrenamiento', marker='x')
plt.plot(epochs_range, val_accuracies, label='Validaci√≥n', marker='x')
plt.title('Accuracy vs. No. of epochs')
plt.xlabel('√âpoca')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)

# --- Loss ---
plt.subplot(1,2,2)
plt.plot(epochs_range, train_losses, label='Entrenamiento', marker='x')
plt.plot(epochs_range, val_losses, label='Validaci√≥n', marker='x')
plt.title('Loss vs. No. of epochs')
plt.xlabel('√âpoca')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()

from sklearn.metrics import f1_score
f1_macro = f1_score(all_labels, all_preds, average='macro')
print(f"\nF1 Macro Score: {f1_macro:.4f}")

"""## Inferencia de Nuevas Imagenes"""

infer_model = models.resnet50(weights=None)
infer_model.fc = nn.Sequential(
    nn.Dropout(0.5),
    nn.Linear(num_features, num_classes)
)
infer_model.load_state_dict(
    torch.load("resnet50_trash_final_v3.pt", map_location=DEVICE)
)
infer_model = infer_model.to(DEVICE)
infer_model.eval()

infer_transforms = val_transforms

## funci√≥n para predecir imagen individual
def predict_image(img_path):
    img = Image.open(img_path).convert("RGB")
    x = infer_transforms(img).unsqueeze(0).to(DEVICE)

    with torch.no_grad():
        probs = torch.softmax(infer_model(x), dim=1)[0]
        pred_idx = probs.argmax().item()

    print("\nüß† Predicci√≥n:", full_dataset.classes[pred_idx])
    print("\nProbabilidades:")
    for cls, p in zip(full_dataset.classes, probs):
        print(f"{cls:10s}: {p.item()*100:.2f}%")

    plt.imshow(img)
    plt.title(full_dataset.classes[pred_idx])
    plt.axis("off")
    plt.show()

"""## Prediccion de imagenes"""

predict_image("test/test4.jpeg")

predict_image("test/test3.jpg")

predict_image("test/test6.jpg")

predict_image("test/test1.jpg")

predict_image("test/test7.jpg")